---
title: "Biostat 625 Final Project - Post 9/11 Flight Delay"
subtitle: https://github.com/chelleonis/flight_delay
author: "Ralph Jiang, Xuelin Gu, Allen Li"
date: \today
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
library(dbplyr)
library(data.table)
library(lubridate)
library(plyr)
library(dplyr)
library(readr)
library(bigmemory)
library(biganalytics) 
library(stringr)
library(ggplot2)
library(plotly)
library(chron)
library(lattice)
library(grid)
library(knitr)
```

## Introduction

Delayed flights are a common occurance in the airline industry, with 25 million flights being delayed (greater than 15 minutes) for 20 years of data. The issue of delayed flights is seemingly unpredictable with so many factors preceding a successful flight. For the customer then, it is of increased importance to be able to anticipate what may cause a delay in their flight. To try to shed some light on the issue, we analyzed a large dataset from the post 9/11 era (2001-2008), given by the Bureau of Transportation Statistics. 


#### Calendar Heatmap

To further motivate our study of flight delays, let's take a look at the prevalence of flight delays (defined as a flight that departs more than 15 minutes past the scheduled time) from 2001-2008.
```{r, eval = FALSE, echo = FALSE}
# Code used to generate delay proportions the first time, the resulting data frame gets stored as an R object and can be read back in the future without having to run this code again
data2001 <- read.csv('2001.csv.bz2')
data2002 <- read.csv('2002.csv.bz2')
data2003 <- read.csv('2003.csv.bz2')
data2004 <- read.csv('2004.csv.bz2')
data2005 <- read.csv('2005.csv.bz2')
data2006 <- read.csv('2006.csv.bz2')
data2007 <- read.csv('2007.csv.bz2')
data2008 <- read.csv('2008.csv.bz2')

data2001 <- data2001[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2002 <- data2002[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2003 <- data2003[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2004 <- data2004[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2005 <- data2005[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2006 <- data2006[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2007 <- data2007[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]
data2008 <- data2008[ , c('Year', 'Month', 'DayofMonth', 'DepDelay')]

data1 <- rbind(data2001, data2002, data2003, data2004, data2005, data2006, data2007, data2008)

data1$Date <- ymd(paste0(data1$Year, '/', data1$Month, '/', data1$DayofMonth))
delay_prop <- data1 %>% group_by(Date) %>% summarise(delay_prop = sum(DepDelay >= 15, na.rm = T) / n()) 
saveRDS(delay_prop, "delay_prop.rds")
```

```{r, echo = FALSE}
delay_prop <- readRDS("delay_prop.rds")
g2r <- c("#5CDA4D", "#FEFEAA", "#FF9563", "#FF0000") # Green to red color palette
source("https://raw.githubusercontent.com/iascchen/VisHealth/master/R/calendarHeat.R") # Calendar Heatmap function
calendarHeat(delay_prop$Date[1:1461], delay_prop$delay_prop[1:1461], ncolors=99, color='g2r', varname = "Flight Delay Proportion (2001-2004)")
calendarHeat(delay_prop$Date[1462:2922], delay_prop$delay_prop[1462:2922], ncolors=99, color='g2r', varname = "Flight Delay Proportion (2005-2008)")
```

We can see that it is quite common for over 25% of flights to be delayed on any given day. The proportion of flights delayed consistently gets much worse towards the end of December / beginning of January, especially on the days right before Christmas and right after New Year's. Comparing across seasons, it appears that spring and fall are relatively delay-free, whereas in the summer, we can expect to have at least 30% of flights delayed. Unfortunately, these trends have not improved over time, and if anything, have worsened ever since 2003. Thus, we believe it is of utmost interest to investigate what factors influence the amount of time flights are delayed.


## Methods

Our dataset contains, at its base, 29 variables and 130 million entries of flight data, containing variables such as flight distance, arrival delay, and calendar month. The base dataset is approximately 12 GB in size. 

### Data Cleaning and Pre-processing

We believe a major factor influencing flight delays is the weather. The source of our daily weather data is NCDC Cliamte Data Online. We split the dataset into managable chunks (by year). The weather datasets included 23 variables such as tempearture, precipitation, and wind speed. We merged the base data to the weather data by using a key consisting of the 3 letter airport code, year, month, and day, concatenated together. The relevant code is in the R Markdown file but not the pdf.

```{r, eval = FALSE, echo = FALSE}
# Code to merge flight data with weather data
weather_data <- fread('weather_data.csv')
station_codes <- as.character(weather_data$`STN---`)

# The weather data uses a six digit code to represent a station, this code can be converted to the three letter airport code via http://old.wetterzentrale.de/klima/stnlst.html
station_codes[station_codes == '702730'] <- 'ANC'
station_codes[station_codes == '722020'] <- 'MIA'
station_codes[station_codes == '722030'] <- 'PBI'
station_codes[station_codes == '722050'] <- 'MCO'
station_codes[station_codes == '722060'] <- 'JAX'
station_codes[station_codes == '722108'] <- 'RSW'
station_codes[station_codes == '722110'] <- 'TPA'
station_codes[station_codes == '722190'] <- 'ATL'
station_codes[station_codes == '722280'] <- 'BHM'
station_codes[station_codes == '722310'] <- 'MSY'
station_codes[station_codes == '722430'] <- 'IAH'
station_codes[station_codes == '722440'] <- 'HOU'
station_codes[station_codes == '722530'] <- 'SAT'
station_codes[station_codes == '722540'] <- 'AUS'
station_codes[station_codes == '722590'] <- 'DFW'
station_codes[station_codes == '722780'] <- 'PHX'
station_codes[station_codes == '722880'] <- 'BUR'
station_codes[station_codes == '722900'] <- 'SAN'
station_codes[station_codes == '722950'] <- 'LAX'
station_codes[station_codes == '722977'] <- 'SNA'
station_codes[station_codes == '723060'] <- 'RDU'
station_codes[station_codes == '723140'] <- 'CLT'
station_codes[station_codes == '723270'] <- 'BNA'
station_codes[station_codes == '723650'] <- 'ABQ'
station_codes[station_codes == '723860'] <- 'LAS'
station_codes[station_codes == '724030'] <- 'IAD'
station_codes[station_codes == '724050'] <- 'DCA'
station_codes[station_codes == '724060'] <- 'BWI'
station_codes[station_codes == '724080'] <- 'PHL'
station_codes[station_codes == '724210'] <- 'CVG'
station_codes[station_codes == '724280'] <- 'CMH'
station_codes[station_codes == '724340'] <- 'STL'
station_codes[station_codes == '724380'] <- 'IND'
station_codes[station_codes == '724460'] <- 'MCI'
station_codes[station_codes == '724839'] <- 'SMF'
station_codes[station_codes == '724930'] <- 'OAK'
station_codes[station_codes == '724940'] <- 'SFO'
station_codes[station_codes == '724945'] <- 'SJC'
station_codes[station_codes == '725020'] <- 'EWR'
station_codes[station_codes == '725030'] <- 'LGA'
station_codes[station_codes == '725080'] <- 'BDL'
station_codes[station_codes == '725090'] <- 'BOS'
station_codes[station_codes == '725200'] <- 'PIT'
station_codes[station_codes == '725240'] <- 'CLE'
station_codes[station_codes == '725280'] <- 'BUF'
station_codes[station_codes == '725300'] <- 'ORD'
station_codes[station_codes == '725340'] <- 'MDW'
station_codes[station_codes == '725370'] <- 'DTW'
station_codes[station_codes == '725500'] <- 'OMA'
station_codes[station_codes == '725650'] <- 'DEN'
station_codes[station_codes == '725720'] <- 'SLC'
station_codes[station_codes == '726400'] <- 'MKE'
station_codes[station_codes == '726580'] <- 'MSP'
station_codes[station_codes == '726980'] <- 'PDX'
station_codes[station_codes == '727930'] <- 'SEA'
station_codes[station_codes == '744860'] <- 'JFK'
station_codes[station_codes == '911820'] <- 'HNL'
station_codes[station_codes == '911900'] <- 'OGG'

weather_data$`STN---` <- station_codes
unique_codes <- unique(station_codes)

# Dealing with missing data in the weather dataset
# Variable descriptions are found here: https://www7.ncdc.noaa.gov/CDO/GSOD_DESC.txt
weather_data$TEMP[weather_data$TEMP == 9999.9] <- NA
weather_data$DEWP[weather_data$DEWP == 9999.9] <- NA
weather_data$SLP[weather_data$SLP == 9999.9] <- NA
weather_data$STP[weather_data$STP == 9999.9] <- NA
weather_data$VISIB[weather_data$VISIB == 999.9] <- NA
weather_data$WDSP[weather_data$WDSP == 999.9] <- NA
weather_data$MXSPD[weather_data$MXSPD == 999.9] <- NA
weather_data$GUST[weather_data$GUST == 999.9] <- NA
weather_data$MAX <- gsub("*", "", weather_data$MAX, fixed = T) # Remove irrelevant asterisks from column
weather_data$MAX[weather_data$MAX == 9999.9] <- NA
weather_data$MIN <- gsub("*", "", weather_data$MIN, fixed = T)
weather_data$MIN[weather_data$MIN == 9999.9] <- NA
weather_data$PRCP[weather_data$PRCP == 99.99] <- NA

# Precipitation is reported in various ways for different stations. There is a character (A-G) at the end of each entry that represents over what time frame the precipitation was measured
weather_data$PRCP1 <- as.numeric(substr(weather_data$PRCP, 1, nchar(weather_data$PRCP) - 1)) # Split entry into numeric part and character part
weather_data$PRCP2 <- substr(weather_data$PRCP, nchar(weather_data$PRCP), nchar(weather_data$PRCP))

# These are multipliers that will allow us to compare precipitation across all stations
weather_data$PRCP2[weather_data$PRCP2 == 'A'] <- 4 
weather_data$PRCP2[weather_data$PRCP2 == 'B'] <- 2
weather_data$PRCP2[weather_data$PRCP2 == 'C'] <- 1.333
weather_data$PRCP2[weather_data$PRCP2 == 'D'] <- 1
weather_data$PRCP2[weather_data$PRCP2 == 'G'] <- 1
weather_data$PRCP2[weather_data$PRCP2 == 'H'] <- 0
weather_data$PRCP2[weather_data$PRCP2 == 'I'] <- 0
weather_data$PRCP2 <- as.numeric(weather_data$PRCP2)
weather_data$PRCP <- weather_data$PRCP1 * weather_data$PRCP2

# Setting snow value of 999.9 to 0 instead of NA because most stations do not explicitly report 0 on days without snow, so a 999.9 will typically just mean no snow
weather_data$SNDP[weather_data$SNDP == 999.9] <- 0 

# FRSHTT: 6 digits, indicators for Fog, Rain/Drizzle, Snow/Ice Pellets, Hail, Thunder, Tornado.
# We want to create a separate column for each variable. Since leading zeroes get removed, we pad them back onto the front until each entry is 6 digits, then we take each 1 character substring to form the new columns
weather_data$FRSHTT <- str_pad(weather_data$FRSHTT, width = 6, pad = '0')
weather_data$Fog <- substr(weather_data$FRSHTT, 1, 1)
weather_data$Rain <- substr(weather_data$FRSHTT, 2, 2)
weather_data$Snow <- substr(weather_data$FRSHTT, 3, 3)
weather_data$Hail <- substr(weather_data$FRSHTT, 4, 4)
weather_data$Thunder <- substr(weather_data$FRSHTT, 5, 5)
weather_data$Tornado <- substr(weather_data$FRSHTT, 6, 6)

# Create a key that will allow the weather data to communicate with the flights data
weather_data$key <- paste0(weather_data$'STN---', weather_data$YEARMODA)

# Remove irrelevant columns
weather_data <- subset(weather_data, select = -c(WBAN, V5, V7, V9, V11, V13, V15, FRSHTT, V23, PRCP1, PRCP2))

flights2001 <- read.csv('2001.csv.bz2')
flights2002 <- read.csv('2002.csv.bz2')
flights2003 <- read.csv('2003.csv.bz2')
flights2004 <- read.csv('2004.csv.bz2')
flights2005 <- read.csv('2005.csv.bz2')
flights2006 <- read.csv('2006.csv.bz2')
flights2007 <- read.csv('2007.csv.bz2')
flights2008 <- read.csv('2008.csv.bz2')

# Since we only have weather data for the 58 biggest airports, we have to filter down to flights whose origin and destination are both one of those 58 airports
flights2001 <- flights2001[flights2001$Origin %in% unique_codes & flights2001$Dest %in% unique_codes, ]
flights2002 <- flights2002[flights2002$Origin %in% unique_codes & flights2002$Dest %in% unique_codes, ]
flights2003 <- flights2003[flights2003$Origin %in% unique_codes & flights2003$Dest %in% unique_codes, ]
flights2004 <- flights2004[flights2004$Origin %in% unique_codes & flights2004$Dest %in% unique_codes, ]
flights2005 <- flights2005[flights2005$Origin %in% unique_codes & flights2005$Dest %in% unique_codes, ]
flights2006 <- flights2006[flights2006$Origin %in% unique_codes & flights2006$Dest %in% unique_codes, ]
flights2007 <- flights2007[flights2007$Origin %in% unique_codes & flights2007$Dest %in% unique_codes, ]
flights2008 <- flights2008[flights2008$Origin %in% unique_codes & flights2008$Dest %in% unique_codes, ]

# Creating two keys (one for origin, one for destination) from the flights dataset in the same format as the previously created key from the weather dataset. We need all days to be two digits long, so we may have to pad a leading zero on
flights2001$key <- paste0(flights2001$Origin, flights2001$Year, str_pad(flights2001$Month, width = 2, pad = '0'), str_pad(flights2001$DayofMonth, width = 2, pad = '0'))
flights2001$key2 <- paste0(flights2001$Dest, flights2001$Year, str_pad(flights2001$Month, width = 2, pad = '0'),
str_pad(flights2001$DayofMonth, width = 2, pad = '0'))
flights2002$key <- paste0(flights2002$Origin, flights2002$Year, str_pad(flights2002$Month, width = 2, pad = '0'),
str_pad(flights2002$DayofMonth, width = 2, pad = '0'))
flights2002$key2 <- paste0(flights2002$Dest, flights2002$Year, str_pad(flights2002$Month, width = 2, pad = '0'),
str_pad(flights2002$DayofMonth, width = 2, pad = '0'))
flights2003$key <- paste0(flights2003$Origin, flights2003$Year, str_pad(flights2003$Month, width = 2, pad = '0'),
str_pad(flights2003$DayofMonth, width = 2, pad = '0'))
flights2003$key2 <- paste0(flights2003$Dest, flights2003$Year, str_pad(flights2003$Month, width = 2, pad = '0'),
str_pad(flights2003$DayofMonth, width = 2, pad = '0'))
flights2004$key <- paste0(flights2004$Origin, flights2004$Year, str_pad(flights2004$Month, width = 2, pad = '0'),
str_pad(flights2004$DayofMonth, width = 2, pad = '0'))
flights2004$key2 <- paste0(flights2004$Dest, flights2004$Year, str_pad(flights2004$Month, width = 2, pad = '0'),
str_pad(flights2004$DayofMonth, width = 2, pad = '0'))
flights2005$key <- paste0(flights2005$Origin, flights2005$Year, str_pad(flights2005$Month, width = 2, pad = '0'),
str_pad(flights2005$DayofMonth, width = 2, pad = '0'))
flights2005$key2 <- paste0(flights2005$Dest, flights2005$Year, str_pad(flights2005$Month, width = 2, pad = '0'),
str_pad(flights2005$DayofMonth, width = 2, pad = '0'))
flights2006$key <- paste0(flights2006$Origin, flights2006$Year, str_pad(flights2006$Month, width = 2, pad = '0'),
str_pad(flights2006$DayofMonth, width = 2, pad = '0'))
flights2006$key2 <- paste0(flights2006$Dest, flights2006$Year, str_pad(flights2006$Month, width = 2, pad = '0'),
str_pad(flights2006$DayofMonth, width = 2, pad = '0'))
flights2007$key <- paste0(flights2007$Origin, flights2007$Year, str_pad(flights2007$Month, width = 2, pad = '0'),
str_pad(flights2007$DayofMonth, width = 2, pad = '0'))
flights2007$key2 <- paste0(flights2007$Dest, flights2007$Year, str_pad(flights2007$Month, width = 2, pad = '0'),
str_pad(flights2007$DayofMonth, width = 2, pad = '0'))
flights2008$key <- paste0(flights2008$Origin, flights2008$Year, str_pad(flights2008$Month, width = 2, pad = '0'),
str_pad(flights2008$DayofMonth, width = 2, pad = '0'))
flights2008$key2 <- paste0(flights2008$Dest, flights2008$Year, str_pad(flights2008$Month, width = 2, pad = '0'),
str_pad(flights2008$DayofMonth, width = 2, pad = '0'))

# Finally, we can merge the files together. The first merge is on the weather at the origin airport, the second merge is on the weather at the destination airport.
flights_weather2001 <- merge(flights2001, weather_data, by = 'key')
flights_weather2001 <- merge(flights_weather2001, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2002 <- merge(flights2002, weather_data, by = 'key')
flights_weather2002 <- merge(flights_weather2002, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2003 <- merge(flights2003, weather_data, by = 'key')
flights_weather2003 <- merge(flights_weather2003, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2004 <- merge(flights2004, weather_data, by = 'key')
flights_weather2004 <- merge(flights_weather2004, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2005 <- merge(flights2005, weather_data, by = 'key')
flights_weather2005 <- merge(flights_weather2005, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2006 <- merge(flights2006, weather_data, by = 'key')
flights_weather2006 <- merge(flights_weather2006, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2007 <- merge(flights2007, weather_data, by = 'key')
flights_weather2007 <- merge(flights_weather2007, weather_data, by.x = 'key2', by.y = 'key')
flights_weather2008 <- merge(flights2008, weather_data, by = 'key')
flights_weather2008 <- merge(flights_weather2008, weather_data, by.x = 'key2', by.y = 'key')

# The output is one dataset for each year, consisting of the original flight data, the weather at the origin airport, and the weather at the destination airport (72 variables total)
fwrite(flights_weather2001, 'flights_weather2001.csv')
fwrite(flights_weather2002, 'flights_weather2002.csv')
fwrite(flights_weather2003, 'flights_weather2003.csv')
fwrite(flights_weather2004, 'flights_weather2004.csv')
fwrite(flights_weather2005, 'flights_weather2005.csv')
fwrite(flights_weather2006, 'flights_weather2006.csv')
fwrite(flights_weather2007, 'flights_weather2007.csv')
fwrite(flights_weather2008, 'flights_weather2008.csv')
```

After the merge, we combined these separate datafiles as follows:  

```{r, eval = FALSE}
##read separate files in the dataset folder and conbine their rows 
dataFiles = list.files(pattern = "*.csv") %>% 
  lapply(read.csv, stringsAsFactors=F) %>% 
  bind_rows 
write.csv(dataFiles,file='out.csv')
```

The operation above was performed in the biostatisitics computing cluster using bash scripts, as the operation was too big to perform in Rstudio due to it's memory limits.  


All in all, our dataset contains 72 variables, which we then trimmed in the subsequent steps. First, we deleted the cancelled and diverted flights that may have different situations with other common delayed flights. Second, to make the analysis more efficient, we removed variables meeting the following criteria:   
1. Provides little information on flight delays
2. Provides information contained in other variables
3. Not included in this project objectives
4. Variable has columns with only 'NA'
5. Variables with highly missing data  

Third, we add a covariate named "Season" (i.e. Fall, Spring) based on the "Month" value.  
Snippets of our data cleaning can be found in the Rmarkdown file. 
```{r, eval = FALSE, echo = FALSE}
list_file = data.table::fread("out.csv")
##control possible confounders 
dat = filter(list_file, Cancelled==0&Diverted==0)
##exclude nonsense indices
dat = subset(dat, -c(V1))

##exclude irrelavent covariates
dat = select(dat,-c(key2, key, DepTime, `STN---.x`, YEARMODA.x, `STN---.y`, YEARMODA.y,    
                    CRSDepTime, ArrTime, CRSArrTime, ActualElapsedTime, CRSElapsedTime, 
                    Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, 
                    NASDelay, SecurityDelay, LateAircraftDelay, STP.x, STP.y, GUST.x, GUST.y))
#create season covariates (not all included)
dat = mutate(dat, season = NA)
dat[which(dat$Month==1|dat$Month==2|dat$Month==3),]$season="winter"
dat[which(dat$Month==4|dat$Month==5|dat$Month==6),]$season="spring"
```

## Data Importing 

Data was loaded in a variety of manners. For the parts we could break down by year, either
read.table or read.csv were used.
```{r,eval = FALSE}
fpath <- file.path(path,"flight_weather_cleaned.csv")
tic("fread 6gb data import")
flight_data <- data.table::fread(fpath)
toc()
#fread 6gb data import: 180.45 sec elapsed
```
For the cleaned data, it takes approximately 3 minutes to import the data for 32 million observations.

```{r, eval = FALSE}
tic("test for read.csv")
x_test <- read.csv("D:/bios625data/flight_weather_cleaned.csv")
toc()
#test for read.csv: 2654.62 sec elapsed
```
Whereas for read.csv, it took 40 minutes to import the data.

The bigmemory package had troubles with loading a 9 GB file on a intel i7-6600U, 2.40 GHz, with 8GB memory, where it would crash Rstudio midway through. The analyses including that package and biganalytics were performed in the biostatistics computing cluster.

## Analytics

### Descriptive Statistics

To preview our data, we were interested in obtaining descriptive statistics for each categories. We investigated variables such as the mean, variance, frequency, and correlation, the code of which can be found in the Rmarkdown file.  

```{r, eval = FALSE, echo = FALSE}
##
#Descriptive Statistics of Variables: (NOT ALL CODE INCLUDED)
select(dat, c(Year, Month, DayofMonth, DayOfWeek, Distance, TaxiIn, TaxiOut, TEMP.x, DEWP.x, SLP.x, VISIB.x,
              WDSP.x, MXSPD.x, MAX.x, MIN.x, PRCP.x, SNDP.x, TEMP.y, DEWP.y, SLP.y, VISIB.y,
              WDSP.y, MXSPD.y, MAX.y, MIN.y, PRCP.y, SNDP.y)) %>% summary()

##obtain frequency tables for categorical covaraites (NOT ALL CATEGORIES INCLUDED)
select(dat, UniqueCarrier) %>% table()
select(dat, FlightNum) %>% table() 

##obtain correlation coefficients of numeric covariates using complete data
select(dat, c(Year, Month, DayofMonth, DayOfWeek, Distance, TaxiIn, TaxiOut, TEMP.x, DEWP.x, SLP.x, VISIB.x, WDSP.x, MXSPD.x, MAX.x, MIN.x, PRCP.x, SNDP.x, TEMP.y, DEWP.y, SLP.y, VISIB.y, WDSP.y, MXSPD.y, MAX.y, MIN.y, PRCP.y, SNDP.y)) %>% cor(,use = "complete.obs")
```

From our results, most varirables had low correlation with each other. Some factors such as raw temperature and maximum temperature were highly correlated, but as a result of using the same information (reword...). The only other noticably highly correlated pair was temperature and dew point, which we deemed to be not a large issue. 

### Linear regression

Linear Regresion was performed with the biganalytics package, where we analyzed delays for departures and arrivals in their own models. The code for arrival delays is included below.  

```{r, eval = FALSE}
#certain variables were chosen to be categorical via as.factor()
#Departure Delays was calculated similarly, but with weather variables coded as .x (the starting destination) instead of .y

Arr_result = biglm.big.matrix (ArrDelay ~ Year + Month + DayofMonth + DayOfWeek + Distance + TEMP.y + DEWP.y + SLP.y + VISIB.y + WDSP.y + MXSPD.y + PRCP.y + SNDP.y + UniqueCarrier + Origin + Dest + Fog.y + Rain.y + Snow.y + Hail.y + Thunder.y + Tornado.y + season, data = dat )

```

Full results of the output can be found in the xlsx file "LM result for all covariates".  


```{r,eval = FALSE, echo = FALSE}
#####obtain GVIF for multilinearity diagnosis
VIF = function(lr_result){
  v = vcov(lr_result)
  assign <- lr_result$assign
  if (names(coefficients(lr_result)[1]) == "(Intercept)") {
    v <- v[-1, -1]
    assign <- assign[-1]
  }
  terms <- labels(terms(lr_result))
  n.terms <- length(terms)
  R <- cov2cor(v)
  detR <- det(R)
  result <- matrix(0, n.terms, 3)
  rownames(result) <- terms
  colnames(result) <- c("GVIF", "Df", "GVIF^(1/(2*Df))")
  for (term in 1:n.terms) {
    subs <- which(assign == term)
    result[term, 1] <- det(as.matrix(R[subs, subs])) *
      det(as.matrix(R[-subs, -subs])) / detR
    result[term, 2] <- length(subs)
  }
  if (all(result[, 2] == 1)) result <- result[, 1] else result[, 3] <- result[, 1]^(1/(2 * result[, 2]))
}

VIF(lr_result)
VIF(Dep_result)
VIF(Arr_result)
#######
```

We additionally performed diagnoses for our model for issues with multicolinearity and overfitting (cross-validation). The method can be found in the Rmarkdown file or the file "residual_diagnostics.R". Our GVIF and PRESS results can be found in the file "lm VIF results.txt"

```{r, eval = FALSE, echo = FALSE}
##create k folds to do cross validation
flds <- createFolds(dat, k = 150, list = TRUE, returnTrain = FALSE)
n=length(flds)
##functions for obtain fitted values based of estimated coefficients
##producing dummy variables
factorize=function(name,data=dat){
  len = nrow(dat)
  q = levels(name)
  q = sort(q)
  group_num = length(q)
  x_matrix = matrix(0, len, group_num)
  ###produce indicate variable for each group of the categorical variable x
  for (i in 1:group_num) {
    x_matrix[,i] = name==q[i]
  }
  x_matrix=x_matrix[,-1]
  return(x_matrix)
}
###PRESS
sum_rresidual=0
SSY=0
Ybar=mean(dat$DepDelay)
n1=0
b = cbind(rep(1,nrow(dat)),factorize(dat$Year), dat$Month, dat$DayofMonth, factorize(dat$DayOfWeek), dat$Distance, dat$TaxiIn, 
          dat$TaxiOut, dat$TEMP.x,dat$DEWP.x, dat$SLP.x, dat$VISIB.x, dat$WDSP.x, dat$MXSPD.x, dat$PRCP.x, dat$SNDP.x, dat$TEMP.y, 
          dat$DEWP.y,dat$SLP.y, dat$VISIB.y, dat$WDSP.y, dat$MXSPD.y, dat$PRCP.y, dat$SNDP.y, factorize(dat$UniqueCarrier), 
          factorize(dat$Origin),factorize(dat$Dest),factorize(dat$Fog.x), factorize(dat$Rain.x), factorize(dat$Snow.x), 
          factorize(dat$Hail.x),factorize(dat$Thunder.x), factorize(dat$Tornado.x), factorize(dat$Fog.y), factorize(dat$Rain.y), 
          factorize(dat$Snow.y),factorize(dat$Hail.y), factorize(dat$Thunder.y), factorize(dat$Tornado.y),factorize(dat$season))

for (i in 1:n){
  a = biglm.big.matrix (DepDelay ~ Year + Month + DayofMonth + DayOfWeek + Distance + TaxiIn + TaxiOut + TEMP.x +
                    DEWP.x + SLP.x + VISIB.x + WDSP.x + MXSPD.x + PRCP.x + SNDP.x + TEMP.y + DEWP.y +
                    SLP.y + VISIB.y + WDSP.y + MXSPD.y + PRCP.y + SNDP.y + UniqueCarrier + Origin + Dest +
                    Fog.x + Rain.x + Snow.x + Hail.x + Thunder.x + Tornado.x + Fog.y + Rain.y + Snow.y +
                    Hail.y + Thunder.y + Tornado.y + season, data = dat[-flds[[i]],] )
  x=b[flds[[i]],]
  c=x%*%summary(a)[[2]][,1]
  c=dat[flds[[i]],]$DepDelay-c
  SSY=SSY+sum((dat[flds[[i]],]$DepDelay-Ybar)^2)
  n1=n1+length(c)
  sum_rresidual=sum_rresidual+sum(c^2)
}
R_PRESS=1-sum_rresidual/SSY
R_PRESS

####PRESS for DepDelay
sum_rresidual=0
SSY=0
Ybar=mean(dat$DepDelay)
n1=0
b=cbind(rep(1,nrow(dat)),factorize(dat$Year), dat$Month, dat$DayofMonth, factorize(dat$DayOfWeek), dat$Distance, dat$TEMP.x,
        dat$DEWP.x, dat$SLP.x, dat$VISIB.x, dat$WDSP.x, dat$MXSPD.x, dat$PRCP.x, dat$SNDP.x, factorize(dat$UniqueCarrier),
        factorize(dat$Origin),factorize(dat$Dest),factorize(dat$Fog.x), factorize(dat$Rain.x), factorize(dat$Snow.x), 
        factorize(dat$Hail.x),factorize(dat$Thunder.x), factorize(dat$Tornado.x), factorize(dat$season))
for (i in 1:n){
a = biglm.big.matrix (DepDelay ~ Year + Month + DayofMonth + DayOfWeek + Distance + TEMP.x + DEWP.x + SLP.x + VISIB.x + WDSP.x + MXSPD.x + PRCP.x + SNDP.x + UniqueCarrier + Origin + Dest + Fog.x + Rain.x + Snow.x + Hail.x + Thunder.x + Tornado.x + season, data = dat[-flds[[i]],] )
x=b[flds[[i]],]
c=x%*%summary(a)[[2]][,1]
c=dat[flds[[i]],]$DepDelay-c
SSY=SSY+sum((dat[flds[[i]],]$DepDelay-Ybar)^2)
n1=n1+length(c)
sum_rresidual=sum_rresidual+sum(c^2)
}
R_PRESS=1-sum_rresidual/SSY
R_PRESS

###PRESS for ArrDelay
sum_rresidual=0
n1=0
Ybar=mean(dat$ArrDelay)
SSY=0
b=cbind(rep(1,nrow(dat)),factorize(dat$Year), dat$Month, dat$DayofMonth, factorize(dat$DayOfWeek), dat$Distance, dat$TEMP.y, dat$DEWP.y,
        dat$SLP.y, dat$VISIB.y, dat$WDSP.y, dat$MXSPD.y, dat$PRCP.y, dat$SNDP.y, factorize(dat$UniqueCarrier), factorize(dat$Origin),
        factorize(dat$Dest), factorize(dat$Fog.y), factorize(dat$Rain.y), factorize(dat$Snow.y),
        factorize(dat$Hail.y), factorize(dat$Thunder.y), factorize(dat$Tornado.y),factorize(dat$season))


for (i in 1:n){
a = biglm.big.matrix (ArrDelay ~ Year + Month + DayofMonth + DayOfWeek + Distance + TEMP.y + DEWP.y + SLP.y + VISIB.y + WDSP.y + MXSPD.y + PRCP.y + SNDP.y + UniqueCarrier + Origin + Dest + Fog.y + Rain.y + Snow.y + Hail.y + Thunder.y + Tornado.y + season, data = dat[-flds[[i]],] )
x=b[flds[[i]],]
c=x%*%summary(a)[[2]][,1]
c=dat[flds[[i]],]$ArrDelay-c
SSY=SSY+sum((dat[flds[[i]],]$ArrDelay-Ybar)^2)
n1=n1+length(c)
sum_rresidual=sum_rresidual+sum(c^2)
}
R_PRESS=1-sum_rresidual/SSY
R_PRESS

```

From our diagnosis, we are interested in VIF values greater than 10 (the typical cutoff). After scaling the values to account for degrees of freedom in categories, we found that none of the covariates had an overwhelmingly large variance inflation factor. Thus, for our linear model we are not concerned with issues of multicolinearity. Our PRESS statistic, 

### Linear Mixed Models

We encountered a memory issue with running the lme4 package on our imported data: 
> "Error: cannot allocate vector of size 256.0 Mb" on Rstudio

We ran the analysis on the biostatistics computing cluster for both arrival and departure models on a select few variables we had interest in.

```{r, eval = FALSE}
#Departure Delay is calculated similarly
library(car)

flight_model2 <- lmer(ArrDelay ~ Year + Fog.x + Rain.x + Snow.x + Hail.x + Thunder.x +
                       TEMP.x + (1 | Year), dat)
vif(flight_model2)
#LMM model runtime: 671.962 sec elapsed
```

Our models took approximately 10 minutes to run on the biostatistics cluster.  

## Results  

### Linear Regression

Note: Not all outputs had a p-value attached to it.

Time Trends:  

Year (Ref 2001) | Estimate  | Standard Error | Day of Week (Ref Sunday) | Estimate | Standard Error
---------- | ------- | ------- | --------- | --------- | --------- 
2002 | -2.6618 | 0.0274 | Monday | -1.9609  | 0.0232
2003 | -2.7028 | 0.027 | Tuesday | -1.0263 | 0.0232
2004 | 0.06525 | 0.0262 | Wednsday | 1.3326 | 0.0231
2005 | 1.9198  | 0.0269 | Thursday | 1.9471 | 0.0231
2006 | 3.6907  | 0.0273 | Friday | -4.3095 | 0.0241
2007 | 5.3389  | 0.0272 | Saturday | -0.3095 | 0.0235
2008 | 3.1277  | 0.0276 | | |

Over the years, it seems as if the delays are more prominent than in previous years. Interestingly enough, the amount of delays is highest at the middle of the week, compared to the expectation of the weekend flight congestion.

Weather Variables: 

Variable | Estimate | Standard Error 
-------- | -------- | ------- | 
Temperature | -0.0868 | 0.0012 
Fog | -0.5347 | 0.0211
Rain |3.057 | 0.0177
Snow | 3.5711 | 0.0339
Hail | 2.3719 | 0.1469
Thunder | 5.9924 | 0.026  

For weather related delays in departures, it seems as if non-clear weather conditions effect the arrival of the planes as expected.  

Departure delays were largely in part similar to arrival delays, with differences in the storm (weather) related variables being slightly more pronounced for the arrival delays. We suspect that flights that are delayed on arrival will effect the departure times of the next flight, creating a chain reaction of sorts for delays.  


### Linear Mixed Models

For our choice of model, we decide to model , with year and intercept as our random effects. The full output is given in "mixed model results and VIFs.txt", but select varirables with high t-values are shown below: 

Variable | Estimate | Standard Error  | t-value
-------- | -------- | ------- | ------
Snow | 9.932 | 0.029 | 340.090
Fog.x1 |3.6959687 | 0.0171 | 215.239
Rain.x1 | 4.5675803 | 0.0152 | 299.109
Thunder.x1 | 8.8340962 | 0.0243466 | 362.847
TEMP.x | 0.0192699 | 0.0004031 | 47.810

The year trend is similar to that of the simple linear regression model, where for our year range, flights on average are more delayed as the years progress. While this trend is not necessarily linear, it indicates to us that 

Additionally, the adverse weather conditions (such as thunder or fog) seem to be a significant factor in causing delays in flights. 

For our random effects, our results for our year variable are quite low, 0.3825, which inidcates to us that our model results don't vary much between the years.  

In general, the model has similar estimates for the year terms for both arrivals and delays. This model however, has much larger effects for weather on arrival delays compared to departure delays. We did not encounter problems with multicolinearity with our model (VIF < 10).


## Conclusion and Further Work

Flight delays can usually be indicated by adverse weather conditions and the timing of the week. While these results are not absolute, customers should double check the weather on the day of the flight to predict delays and plan on flying during the beginning of the week to reduce chances of delays. As our model contains an excessive amount of variables, we may have ran into a problem with overfitting, as inidicated by our cross-validation results. We believe an implementation of model fitting techniques would be helpful for the inference of these models and moving forward with investigation on a larger more recent dataset of the current decade. 

Our work was comprehensive, but not exhaustive, with the following topics of interest for future investigation  
* Pre 9/11 Era comparison  
  + As our analysis contains information on post-9/11, where security measures have been tightened signficantly, we would like to see if effects that are  signficant in causing delays in this era are more pronounced before 9/11   
* Compilation of more recent Data (2009-present)    
* Full dataset (1987-2008) would be too large for our methods used (greater than 20 GB)
  + Having computation troubles as seen above, may need different forms  
  + Would require different forms of data storage  
  + Look into using RHadoop   
  + Not limited to cluster computing  
* Investigation of different models   
* Dealing with missing data on a large scale  
* Expanded Linear Mixed Model Analysis  

## Special Acknowledgements

Special Thanks to Daniel Barker for his help and availibility in teaching us how to use the computing clusters. 

